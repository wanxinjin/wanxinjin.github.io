I"ï=<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>
<hr />
<p style="margin-bottom:0.0cm; margin-left: 1.5cm"> </p>

<h4 id="table-of-contents">Table of Contents</h4>

<ul>
  <li><a href="#HybridReduction">Task-Driven Hybrid Model Reduction for Dexterous Manipulation</a></li>
  <li><a href="#LFDC">Learning from Human Directional Corrections</a></li>
  <li><a href="#LFSD">Learning from Sparse Demonstrations</a></li>
  <li><a href="#SafePDP">Safe Pontryagin Differentiable Programming</a></li>
  <li><a href="#PDP">Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework</a></li>
</ul>

<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>
<p><a name="HybridReduction"></a> 
<br /></p>

<hr />

<h3 id="task-driven-hybrid-model-reduction-for-dexterous-manipulation"><a href="../td_hybridreduction"><b>Task-Driven Hybrid Model Reduction for Dexterous Manipulation</b></a></h3>
<p style="margin-bottom:0.7cm; margin-left: 1.5cm"> </p>

<center>
<p style="margin-bottom:0.5cm; margin-left: 0.0cm">
<img src="../blogs/TRO_HybridReduction/figures/turning_webpage2.gif" width="350" />
<img src="../blogs/TRO_HybridReduction/figures/moving_webpage2.gif" width="350" />
</p>
</center>

<blockquote>
  <h6 id="arxiv-tbd">Arxiv: TBD</h6>
  <h6 id="code-python-httpsgithubcomwanxinjintask-driven-hybrid-reduction">Code (Python): <a href="https://github.com/wanxinjin/Task-Driven-Hybrid-Reduction">https://github.com/wanxinjin/Task-Driven-Hybrid-Reduction</a></h6>
  <h6 id="webpage-tbd">Webpage: TBD</h6>
  <details> <summary><span style="font-family:Georgia; font-size:0.9em;">Abstract (click to check abstract)</span></summary>
In contact-rich tasks, like dexterous manipulation, the hybrid nature of
making and breaking contact creates challenges for model representation and
control. For example, choosing and sequencing contact locations for in-hand
manipulation, where there are thousands of potential hybrid modes, is not
generally tractable. In this paper, we are inspired by the observation that far
fewer modes are actually necessary to accomplish many tasks. Building on our
prior work learning hybrid models, represented as linear complementarity
systems, we find a reduced-order hybrid model requiring only a limited number
of task-relevant modes. This simplified representation, in combination with
model predictive control, enables real-time control yet is sufficient for
achieving high performance. We demonstrate the proposed method first on
synthetic hybrid systems, reducing the mode count by multiple orders of
magnitude while achieving task performance loss of less than 5%. We also apply
the proposed method to a three-fingered robotic hand manipulating a previously
unknown object. With no prior knowledge, we achieve state-of-the-art
closed-loop performance in less than five minutes of online learning.
</details>
</blockquote>

<p style="margin-bottom:3.0cm; margin-left: 1.5cm"> </p>
<p><a name="LFDC"></a> 
<br /></p>

<hr />

<h3 id="-learning-from-human-directional-corrections-"><b> Learning from Human Directional Corrections </b></h3>
<p style="margin-bottom:0.7cm; margin-left: 1.5cm"> </p>

<center>
<iframe width="700" height="400" src="https://www.youtube.com/embed/Mwlwt055Tgg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</center>

<p style="margin-bottom:0.5cm; margin-left: 1.5cm"> </p>

<blockquote>
  <h6 id="arxiv-httpsarxivorgabs201115014">Arxiv: <a href="https://arxiv.org/abs/2011.15014" target="_blank">https://arxiv.org/abs/2011.15014</a></h6>
  <h6 id="code-python-httpsgithubcomwanxinjinlearning-from-directional-corrections">Code (Python): <a href="https://github.com/wanxinjin/Learning-from-Directional-Corrections" target="_blank">https://github.com/wanxinjin/Learning-from-Directional-Corrections</a></h6>
  <details> <summary><span style="font-family:Georgia; font-size:0.9em;">Abstract (click to show abstract)</span></summary>
This paper proposes a novel approach that enables a robot to learn an objective function incrementally from human directional corrections. Existing methods learn from human magnitude corrections; since a human needs to carefully choose the magnitude of each correction, those methods can easily lead to over-corrections and learning inefficiency. The proposed method only requires human directional corrections -- corrections that only indicate the direction of an input change without indicating its magnitude. We only assume that each correction, regardless of its magnitude, points in a direction that improves the robot's current motion relative to an unknown objective function. The allowable corrections satisfying this assumption account for half of the input space, as opposed to the magnitude corrections which have to lie in a shrinking level set. For each directional correction, the proposed method updates the estimate of the objective function based on a cutting plane method, which has a geometric interpretation. We have established theoretical results to show the convergence of the learning process. The proposed method has been tested in numerical examples, a user study on two human-robot games, and a real-world quadrotor experiment. The results confirm the convergence of the proposed method and further show that the method is significantly more effective (higher success rate), efficient/effortless (less human corrections needed), and potentially more accessible (fewer early wasted trials) than the state-of-the-art robot learning frameworks.
</details>
</blockquote>

<p><a name="LFSD"></a></p>

<p style="margin-bottom:3.0cm; margin-left: 1.5cm"> </p>
<p><br /></p>

<hr />

<h3 id="-learning-from-sparse-demonstrations-"><b> Learning from Sparse Demonstrations </b></h3>
<p style="margin-bottom:0.7cm; margin-left: 1.5cm"> </p>

<center>
<iframe width="700" height="400" src="https://www.youtube.com/embed/BYAsqMxW5Z4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</center>

<p style="margin-bottom:0.5cm; margin-left: 1.5cm"> </p>

<blockquote>
  <h6 id="arxiv-httpsarxivorgabs200802159">Arxiv: <a href="https://arxiv.org/abs/2008.02159" target="_blank">https://arxiv.org/abs/2008.02159</a></h6>
  <h6 id="code-python-httpsgithubcomwanxinjinlearning-from-sparse-demonstrations">Code (Python): <a href="https://github.com/wanxinjin/Learning-from-Sparse-Demonstrations" target="_blank">https://github.com/wanxinjin/Learning-from-Sparse-Demonstrations</a></h6>
  <details> <summary><span style="font-family:Georgia; font-size:0.9em;">Abstract (click to show abstract)</span></summary>
This paper develops the method of Continuous Pontryagin Differentiable Programming (Continuous PDP), which enables a robot to learn an objective function from a few sparsely demonstrated keyframes. The keyframes, labeled with some time stamps, are the desired task-space outputs, which a robot is expected to follow sequentially. The time stamps of the keyframes can be different from the time of the robot's actual execution. The method jointly finds an objective function and a time-warping function such that the robot's resulting trajectory sequentially follows the keyframes with minimal discrepancy loss. The Continuous PDP minimizes the discrepancy loss using projected gradient descent, by efficiently solving the gradient of the robot trajectory with respect to the unknown parameters. The method is first evaluated on a simulated robot arm and then applied to a 6-DoF quadrotor to learn an objective function for motion planning in unmodeled environments. The results show the efficiency of the method, its ability to handle time misalignment between keyframes and robot execution, and the generalization of objective learning into unseen motion conditions.
</details>
</blockquote>

<p style="margin-bottom:3.0cm; margin-left: 1.5cm"> </p>
<p><a name="PDP"></a> 
<br /></p>

<hr />

<h3 id="-pontryagin-differentiable-programming-an-end-to-end-learning-and-control-framework-"><b> Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework </b></h3>

<p style="margin-bottom:0.7cm; margin-left: 0.5cm"> </p>

<h4 id="neurips-2020-presentation">NeurIPS 2020 Presentation</h4>
<p style="margin-bottom:0.5cm; margin-left: 0.5cm"> </p>
<center>
	<div id="presentation-embed-38936632"></div>
<script src="https://slideslive.com/embed_presentation.js"></script>
<script>
  embed = new SlidesLiveEmbed("presentation-embed-38936632", {
    presentationId: "38936632",
    autoPlay: false,
    verticalEnabled: true,
  });
</script>
</center>

<p style="margin-bottom:1.7cm; margin-left: 0.5cm"> </p>

<h5 id="examples-of-using-pdp-to-solve-robotic-tasks">Examples of using PDP to solve robotic tasks</h5>
<p style="margin-bottom:0.5cm; margin-left: 0.5cm"> </p>

<div class="row">
    <div class="col-sm mt-2 mt-md-0">
<iframe width="360" height="202" src="https://www.youtube.com/embed/5Jsu772Sqcg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        <div class="caption">
            PDP for optimal control.
        </div>
    </div>

        <div class="col-sm mt-2 mt-md-0">
<iframe width="360" height="202" src="https://www.youtube.com/embed/awVNiCIJCfs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        <div class="caption">
            PDP for inverse reinforcement learning.
        </div>
    </div>

</div>

<blockquote>
  <h6 id="arxiv-httpsarxivorgpdf191212970pdf">Arxiv: <a href="https://arxiv.org/pdf/1912.12970.pdf" target="_blank">https://arxiv.org/pdf/1912.12970.pdf</a></h6>
  <h6 id="code-python-httpsgithubcomwanxinjinpontryagin-differentiable-programming">Code (Python): <a href="https://github.com/wanxinjin/Pontryagin-Differentiable-Programming" target="_blank">https://github.com/wanxinjin/Pontryagin-Differentiable-Programming</a></h6>
  <details> <summary><span style="font-family:Georgia; font-size:0.9em;">Abstract (click to show abstract)</span></summary>
This paper develops a Pontryagin Differentiable Programming (PDP) methodology,
which establishes a unified framework to solve a broad class of learning and control
tasks. The PDP distinguishes from existing methods by two novel techniques: first,
we differentiate through Pontryaginâ€™s Maximum Principle, and this allows to obtain
the analytical derivative of a trajectory with respect to tunable parameters within an
optimal control system, enabling end-to-end learning of dynamics, policies, or/and
control objective functions; and second, we propose an auxiliary control system in
the backward pass of the PDP framework, and the output of this auxiliary control
system is the analytical derivative of the original systemâ€™s trajectory with respect
to the parameters, which can be iteratively solved using standard control tools. We
investigate three learning modes of the PDP: inverse reinforcement learning, system
identification, and control/planning. We demonstrate the capability of the PDP in
each learning mode on different high-dimensional systems, including multi-link
robot arm, 6-DoF maneuvering quadrotor, and 6-DoF rocket powered landing
</details>
</blockquote>

<p style="margin-bottom:2.0cm; margin-left: 1.5cm"> </p>
<hr />
<p style="margin-bottom:2.0cm; margin-left: 1.5cm"> </p>

<h3 id="-safe-pontryagin-differentiable-programming-"><b> Safe Pontryagin Differentiable Programming </b></h3>

<p style="margin-bottom:0.7cm; margin-left: 0.5cm"> </p>

<h5 id="neurips-2021-presentation">NeurIPS 2021 Presentation</h5>

<center>
<div id="presentation-embed-38968248"></div>
<script src="https://slideslive.com/embed_presentation.js"></script>
<script>
  embed = new SlidesLiveEmbed("presentation-embed-38968248", {
    presentationId: "38968248",
    autoPlay: false,
    verticalEnabled: true,
  });
</script>
</center>

<p style="margin-bottom:1.5cm; margin-left: 0.5cm"> </p>

<h5 id="examples-of-using-safe-pdp-to-solve-safety-critical-robotic-tasks">Examples of using Safe-PDP to solve safety-critical robotic tasks</h5>
<p style="margin-bottom:0.5cm; margin-left: 0.5cm"> </p>

<div class="row">
    <div class="col-sm mt-2 mt-md-0">
        <iframe width="360" height="202" src="https://www.youtube.com/embed/sC81qc2ip8U" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        <div class="caption">
            Safe PDP for safe neural policy optimization.
        </div>
    </div>
    <div class="col-sm mt-2 mt-md-0">
          <iframe width="360" height="202" src="https://www.youtube.com/embed/vZVxgo30mDs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
          <div class="caption">
            Safe PDP for safe motion planning.
        </div>
    </div>
</div>

<div class="row">
      <div class="col-sm mt-3 mt-md-0">
    </div>
      <div class="col-sm mt-3 mt-md-0">
        <iframe width="360" height="202" src="https://www.youtube.com/embed/OBiLYYlWi98" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        <div class="caption">
          Safe PDP for learning MPCs (i.e., jointly learning dynamics,  constraints, and control cost) from demonstrations.
      </div>
    </div>
    <div class="col-sm mt-3 mt-md-0">
    </div>
</div>

<blockquote>
  <h6 id="arxiv-httpsarxivorgabs210514937">Arxiv: <a href="https://arxiv.org/abs/2105.14937" target="_blank">https://arxiv.org/abs/2105.14937</a></h6>
  <h6 id="code-python-httpsgithubcomwanxinjinsafe-pdp">Code (Python): <a href="https://github.com/wanxinjin/Safe-PDP" target="_blank">https://github.com/wanxinjin/Safe-PDP</a></h6>
  <details> <summary><span style="font-family:Georgia; font-size:0.9em;">Abstract (click to show abstract)</span></summary>
We propose a Safe Pontryagin Differentiable Programming (Safe PDP) methodology, which establishes a theoretical and algorithmic framework to solve a broad class of safety-critical learning and control tasks -- problems that require the guarantee of safety constraint satisfaction at any stage of the learning and control progress. In the spirit of interior-point methods, Safe PDP handles different types of system constraints on states and inputs by incorporating them into the cost or loss through barrier functions. We prove three fundamentals of the proposed Safe PDP: first, both the solution and its gradient in the backward pass can be approximated by solving their more efficient unconstrained counterparts; second, the approximation for both the solution and its gradient can be controlled for arbitrary accuracy by a barrier parameter; and third, importantly, all intermediate results throughout the approximation and optimization strictly respect the constraints, thus guaranteeing safety throughout the entire learning and control process. We demonstrate the capabilities of Safe PDP in solving various safety-critical tasks, including safe policy optimization, safe motion planning, and learning MPCs from demonstrations, on different challenging systems such as 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing.
</details>
</blockquote>

<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>
<hr />
<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>

:ET